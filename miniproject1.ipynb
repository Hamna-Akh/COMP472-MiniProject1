{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation & Analysis\n",
    "\n",
    "## 1.2 - Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### goemotions.json.gz must be placed in the parent of current directory\n",
    "\n",
    "path = os.getcwd()\n",
    "f = gzip.open(os.path.abspath(os.path.join(path, os.pardir)) + '/goemotions.json.gz', 'rb')\n",
    "file_content = f.read()\n",
    "data_list = json.loads(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 - Extracting the posts and 2 sets of labels (emotion and sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "emotions = []\n",
    "sentiments = []\n",
    "\n",
    "for entry in data_list:\n",
    "    posts.append(entry[0])\n",
    "    emotions.append(entry[1])\n",
    "    sentiments.append(entry[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 - Plotting the distribution of the posts in each category and saving the graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_counts = []\n",
    "emotion_labels = []\n",
    "sentiment_counts = []\n",
    "sentiment_labels = []\n",
    "\n",
    "# Get n different colors\n",
    "def getColors(n):\n",
    "    colors = []\n",
    "    for i in range(n):\n",
    "        color = '#%06X' % randint(0, 0xFFFFFF)\n",
    "        while color in colors:\n",
    "            color = '#%06X' % randint(0, 0xFFFFFF)\n",
    "        colors.append(color)\n",
    "    return colors\n",
    "\n",
    "# Breakdown emotions list into counts and labels\n",
    "for emotion in emotions:\n",
    "    if emotion in emotion_labels:\n",
    "        index = emotion_labels.index(emotion)\n",
    "        emotion_counts[index] = emotion_counts[index] + 1\n",
    "    else:\n",
    "        emotion_labels.append(emotion)\n",
    "        emotion_counts.append(1)\n",
    "\n",
    "# Breakdown sentiments list into counts and labels\n",
    "for sentiment in sentiments:\n",
    "    if sentiment in sentiment_labels:\n",
    "        index = sentiment_labels.index(sentiment)\n",
    "        sentiment_counts[index] = sentiment_counts[index] + 1\n",
    "    else:\n",
    "        sentiment_labels.append(sentiment)\n",
    "        sentiment_counts.append(1)\n",
    "\n",
    "# Calculating frequencies for each classification\n",
    "emotion_freq = [round((count/sum(emotion_counts)), 3) for count in emotion_counts]\n",
    "sentiment_freq = [round((count/sum(sentiment_counts)), 3) for count in sentiment_counts]\n",
    "\n",
    "with PdfPages('frequency_charts.pdf') as pdf:\n",
    "    # Plotting emotion frequencies\n",
    "    plt.pie(emotion_counts, labels=emotion_labels, \n",
    "            startangle=90, colors=getColors(len(emotion_labels)),\n",
    "            rotatelabels=True, counterclock=False, \n",
    "            explode=[0.1 for emotion in emotion_labels], shadow=True)\n",
    "    plt.title('Emotion Frequencies', y=1.25)\n",
    "    # Displaying distribution in legend because hard to see on the chart    \n",
    "    plt.legend(title='Emotions:', labels=[f'{l}: {s:0.1f}%' for l, s in zip(emotion_labels, [freq * 100 for freq in emotion_freq])],\n",
    "            bbox_to_anchor=(1.2,0.5), loc='center right', \n",
    "            bbox_transform=plt.gcf().transFigure)\n",
    "    pdf.savefig(bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Plotting sentiment frequencies\n",
    "    plt.pie(sentiment_counts, labels=sentiment_labels, \n",
    "            startangle=90, colors=getColors(len(sentiment_labels)),\n",
    "            counterclock=False, autopct='%1.1f%%',\n",
    "            explode=[0.1 for sentiment in sentiment_labels], shadow=True)\n",
    "    plt.title('Sentiment Frequencies')\n",
    "    plt.legend(title='Sentiments:', labels=sentiment_labels, bbox_to_anchor=(1,0.5), loc='center right', bbox_transform=plt.gcf().transFigure)\n",
    "    pdf.savefig(bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Words as Features\n",
    "\n",
    "## 2.1 - Processing the dataset: Extracting tokens/words and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  30449 tokens\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(posts)\n",
    "\n",
    "# Vectorizing emotions and sentiments\n",
    "emotion_to_index = {}\n",
    "sentiment_to_index = {}\n",
    "index = 0\n",
    "\n",
    "for emotion in emotions:\n",
    "    if emotion in emotion_to_index:\n",
    "        # already seen\n",
    "        continue\n",
    "    emotion_to_index[emotion] = index\n",
    "    index += 1\n",
    "y_emotions = [emotion_to_index[emotion] for emotion in emotions]\n",
    "\n",
    "index = 0\n",
    "for sentiment in sentiments:\n",
    "    if sentiment in sentiment_to_index:\n",
    "        continue\n",
    "    sentiment_to_index[sentiment] = index\n",
    "    index += 1\n",
    "y_sentiments = [sentiment_to_index[sentiment] for sentiment in sentiments]\n",
    "\n",
    "print('Size of vocabulary: ', len(vectorizer.vocabulary_), 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emotions_train, X_emotions_test, y_emotions_train, y_emotions_test = train_test_split(X, y_emotions, test_size=0.2)\n",
    "X_sentiments_train, X_sentiments_test, y_sentiments_train, y_sentiments_test = train_test_split(X, y_sentiments, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Training and testing classifiers for both classifications, using word frequency as features\n",
    "\n",
    "### 2.3.1 - Base-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_MNB = MultinomialNB()\n",
    "\n",
    "base_MNB_emotions_model = base_MNB.fit(X_emotions_train, y_emotions_train)\n",
    "y_base_MNB_emotions_pred = base_MNB_emotions_model.predict(X_emotions_test)\n",
    "\n",
    "base_MNB_sentiments_model = base_MNB.fit(X_sentiments_train, y_sentiments_train)\n",
    "y_base_MNB_sentiments_pred = base_MNB_sentiments_model.predict(X_sentiments_test)\n",
    "\n",
    "# print(classification_report(y_emotions_test, y_base_MNB_emotions_pred, target_names=emotion_labels))\n",
    "# print(classification_report(y_sentiments_test, y_base_MNB_sentiments_pred, target_names=sentiment_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 - Base-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_DT = DecisionTreeClassifier()\n",
    "\n",
    "base_DT_emotions_model = base_DT.fit(X_emotions_train, y_emotions_train)\n",
    "y_base_DT_emotions_pred = base_DT_emotions_model.predict(X_emotions_test)\n",
    "\n",
    "base_DT_sentiments_model = base_DT.fit(X_sentiments_train, y_sentiments_train)\n",
    "y_base_DT_sentiments_pred = base_DT_sentiments_model.predict(X_sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 - Base-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_MLP = MLPClassifier(max_iter=5) ### talk about low epochs in analysis\n",
    "\n",
    "base_MLP_emotions_model = base_MLP.fit(X_emotions_train, y_emotions_train)\n",
    "y_base_MLP_emotions_pred = base_MLP_emotions_model.predict(X_emotions_test)\n",
    "\n",
    "base_MLP_sentiments_model = base_MLP.fit(X_sentiments_train, y_sentiments_train)\n",
    "y_base_MLP_sentiments_pred = base_MLP_sentiments_model.predict(X_sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 - Top-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params for emotions MNB:  {'alpha': 3.0}\n",
      "best params for sentiments MNB:  {'alpha': 3.0}\n"
     ]
    }
   ],
   "source": [
    "top_MNB_hyper_params = {\n",
    "    # Because an alpha too small will result in numeric errors, 0 is set as 1.0e-10\n",
    "    'alpha': [1.0e-10, 0.5, 1.5, 3.0]\n",
    "}\n",
    "\n",
    "top_MNB_emotions = MultinomialNB(class_prior=emotion_freq)\n",
    "top_MNB_sentiments = MultinomialNB(class_prior=sentiment_freq)\n",
    "\n",
    "top_MNB_emotions_grid_search = GridSearchCV(estimator=top_MNB_emotions, param_grid=top_MNB_hyper_params)\n",
    "top_MNB_sentiments_grid_search = GridSearchCV(estimator=top_MNB_sentiments, param_grid=top_MNB_hyper_params)\n",
    "\n",
    "top_MNB_emotions_model = top_MNB_emotions_grid_search.fit(X_emotions_train, y_emotions_train)\n",
    "y_top_MNB_emotions_pred = top_MNB_emotions_model.predict(X_emotions_test)\n",
    "# print('best params for emotions MNB: ', top_MNB_emotions_model.best_params_)\n",
    "\n",
    "top_MNB_sentiments_model = top_MNB_sentiments_grid_search.fit(X_sentiments_train, y_sentiments_train)\n",
    "y_top_MNB_sentiments_pred = top_MNB_sentiments_model.predict(X_sentiments_test)\n",
    "# print('best params for sentiments MNB: ', top_MNB_sentiments_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 - Top-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params for emotions DT:  {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2}\n",
      "best params for sentiments DT:  {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "top_DT_hyper_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 3, 4]\n",
    "}\n",
    "\n",
    "top_DT = DecisionTreeClassifier()\n",
    "top_DT_grid_search = GridSearchCV(estimator=top_DT, param_grid=top_DT_hyper_params)\n",
    "\n",
    "top_DT_emotions_model = top_DT_grid_search.fit(X_emotions_train, y_emotions_train)\n",
    "y_top_DT_emotions_pred = top_DT_emotions_model.predict(X_emotions_test)\n",
    "\n",
    "top_DT_sentiments_model = top_DT_grid_search.fit(X_sentiments_train, y_sentiments_train)\n",
    "y_top_DT_sentiments_pred = top_DT_sentiments_model.predict(X_sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 - Top-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_MLP_hyper_params = {\n",
    "    'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
    "    'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "    'solver': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "top_MLP = MLPClassifier(max_iter=5) ### talk about low epochs in analysis\n",
    "top_MLP_grid_search = GridSearchCV(estimator=top_MLP, param_grid=top_MLP_hyper_params)\n",
    "\n",
    "top_MLP_emotions_model = top_MLP_grid_search.fit(X_emotions_train, y_emotions_train)\n",
    "y_top_MLP_emotions_pred = top_MLP_emotions_model.predict(X_emotions_test)\n",
    "\n",
    "top_MLP_sentiments_model = top_MLP_grid_search.fit(X_sentiments_train, y_sentiments_train)\n",
    "y_top_MLP_sentiments_pred = top_MLP_sentiments_model.predict(X_sentiments_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
